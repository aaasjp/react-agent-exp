{"query": "deepagents documentation", "follow_up_questions": null, "answer": null, "images": [], "results": [{"url": "https://docs.langchain.com/oss/python/deepagents/overview", "title": "Deep Agents overview - Docs by LangChain", "content": "* When to use deep agents. # Deep Agents overview. Build agents that can plan, use subagents, and leverage file systems for complex tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents. ## â€‹ When to use deep agents. Use deep agents when you need agents that can:. For simpler use cases, consider using LangChainâ€™s `create_agent` or building a custom LangGraph workflow. Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges. A built-in `task` tool enables agents to spawn specialized subagents for context isolation. Deep agents is built on top of:. * LangChain - Tools and model integrations work seamlessly with deep agents. Build your first deep agent## Customization.", "score": 0.83932644, "raw_content": "[Docs by LangChain home page](/)\n\n[LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Deep Agents](/oss/python/deepagents/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/oss/python/learn)[Reference](/oss/python/reference/overview)[Contribute](/oss/python/contributing/overview)\n\n* [Overview](/oss/python/deepagents/overview)\n\n##### Get started\n\n* [Quickstart](/oss/python/deepagents/quickstart)\n* [Customization](/oss/python/deepagents/customization)\n\n##### Core capabilities\n\n* [Agent harness](/oss/python/deepagents/harness)\n* [Backends](/oss/python/deepagents/backends)\n* [Subagents](/oss/python/deepagents/subagents)\n* [Human-in-the-loop](/oss/python/deepagents/human-in-the-loop)\n* [Long-term memory](/oss/python/deepagents/long-term-memory)\n* [Middleware](/oss/python/deepagents/middleware)\n\n##### Command line interface\n\n* [Use the CLI](/oss/python/deepagents/cli)\n\n* [When to use deep agents](#when-to-use-deep-agents)\n* [Core capabilities](#core-capabilities)\n* [Relationship to the LangChain ecosystem](#relationship-to-the-langchain-ecosystem)\n* [Get started](#get-started)\n\n# Deep Agents overview\n\nBuild agents that can plan, use subagents, and leverage file systems for complex tasks\n\n[`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n\n## [â€‹](#when-to-use-deep-agents) When to use deep agents\n\nUse deep agents when you need agents that can:\n\n* **Handle complex, multi-step tasks** that require planning and decomposition\n* **Manage large amounts of context** through file system tools\n* **Delegate work** to specialized subagents for context isolation\n* **Persist memory** across conversations and threads\n\nFor simpler use cases, consider using LangChainâ€™s [`create_agent`](/oss/python/langchain/agents) or building a custom [LangGraph](/oss/python/langgraph/overview) workflow.\n\n## [â€‹](#core-capabilities) Core capabilities\n\n## Planning and task decomposition\n\nDeep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n\n## Context management\n\nFile system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n\n## Subagent spawning\n\nA built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agentâ€™s context clean while still going deep on specific subtasks.\n\n## Long-term memory\n\nExtend agents with persistent memory across threads using LangGraphâ€™s Store. Agents can save and retrieve information from previous conversations.\n\n## [â€‹](#relationship-to-the-langchain-ecosystem) Relationship to the LangChain ecosystem\n\nDeep agents is built on top of:\n\n* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management\n* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents\n* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment\n\nDeep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).\n\n## [â€‹](#get-started) Get started\n\n[## Quickstart\n\nBuild your first deep agent](/oss/python/deepagents/quickstart)[## Customization\n\nLearn about customization options](/oss/python/deepagents/customization)[## Middleware\n\nUnderstand the middleware architecture](/oss/python/deepagents/middleware)[## Reference\n\nSee the `deepagents` API reference](https://reference.langchain.com/python/deepagents/)\n\n---\n\n[Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n\n[Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\n[Quickstart\n\nNext](/oss/python/deepagents/quickstart)"}, {"url": "https://docs.langchain.com/oss/python/deepagents/quickstart", "title": "Quickstart - Docs by LangChain", "content": "* Step 4: Create a deep agent. This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. ### â€‹ Step 3: Create a search tool. import os import  os from typing import Literal from  typing import  Literal from tavily import TavilyClient from  tavily import  TavilyClient from deepagents import create_deep_agent from  deepagents import  create_deep_agent tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"]) tavily_client = TavilyClient(api_key =os.environ[\"TAVILY_API_KEY\"]) def internet_search(def  internet_search( query: str,  query: str, max_results: int = 5,  max_results: int =  5, topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",  topic: Literal[\"general\", \"news\", \"finance\"] =  \"general\", include_raw_content: bool = False,  include_raw_content: bool =  False,):):  \"\"\"Run a web search\"\"\"  \"\"\"Run a web search\"\"\" return tavily_client.search( return tavily_client.search( query, query, max_results=max_results,  max_results =max_results, include_raw_content=include_raw_content,  include_raw_content =include_raw_content, topic=topic,  topic =topic, ) ). ### â€‹ Step 4: Create a deep agent. \"\"\" \"\"\" agent = create_deep_agent(agent = create_deep_agent( tools=[internet_search],  tools =[internet_search], system_prompt=research_instructions  system_prompt = research_instructions)). ### â€‹ Step 5: Run the agent.", "score": 0.8047902, "raw_content": "[Docs by LangChain home page](/)\n\n[LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Deep Agents](/oss/python/deepagents/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/oss/python/learn)[Reference](/oss/python/reference/overview)[Contribute](/oss/python/contributing/overview)\n\n* [Overview](/oss/python/deepagents/overview)\n\n##### Get started\n\n* [Quickstart](/oss/python/deepagents/quickstart)\n* [Customization](/oss/python/deepagents/customization)\n\n##### Core capabilities\n\n* [Agent harness](/oss/python/deepagents/harness)\n* [Backends](/oss/python/deepagents/backends)\n* [Subagents](/oss/python/deepagents/subagents)\n* [Human-in-the-loop](/oss/python/deepagents/human-in-the-loop)\n* [Long-term memory](/oss/python/deepagents/long-term-memory)\n* [Middleware](/oss/python/deepagents/middleware)\n\n##### Command line interface\n\n* [Use the CLI](/oss/python/deepagents/cli)\n\n* [Prerequisites](#prerequisites)\n* [Step 1: Install dependencies](#step-1%3A-install-dependencies)\n* [Step 2: Set up your API keys](#step-2%3A-set-up-your-api-keys)\n* [Step 3: Create a search tool](#step-3%3A-create-a-search-tool)\n* [Step 4: Create a deep agent](#step-4%3A-create-a-deep-agent)\n* [Step 5: Run the agent](#step-5%3A-run-the-agent)\n* [What happened?](#what-happened)\n* [Next steps](#next-steps)\n\n[Get started](/oss/python/deepagents/quickstart)\n\n# Quickstart\n\nBuild your first deep agent in minutes\n\nThis guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. Youâ€™ll build a research agent that can conduct research and write reports.\n\n## [â€‹](#prerequisites) Prerequisites\n\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n\n### [â€‹](#step-1:-install-dependencies) Step 1: Install dependencies\n\nCopy\n\n```\npip install deepagents tavily-python pip  install  deepagents tavily-python\n```\n\n### [â€‹](#step-2:-set-up-your-api-keys) Step 2: Set up your API keys\n\nCopy\n\n```\nexport ANTHROPIC_API_KEY=\"your-api-key\" export  ANTHROPIC_API_KEY =\"your-api-key\"export TAVILY_API_KEY=\"your-tavily-api-key\" export  TAVILY_API_KEY =\"your-tavily-api-key\"\n```\n\n### [â€‹](#step-3:-create-a-search-tool) Step 3: Create a search tool\n\nCopy\n\n```\nimport os import  os from typing import Literal from  typing import  Literal from tavily import TavilyClient from  tavily import  TavilyClient from deepagents import create_deep_agent from  deepagents import  create_deep_agent tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"]) tavily_client = TavilyClient(api_key =os.environ[\"TAVILY_API_KEY\"]) def internet_search(def  internet_search( query: str,  query: str, max_results: int = 5,  max_results: int =  5, topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",  topic: Literal[\"general\", \"news\", \"finance\"] =  \"general\", include_raw_content: bool = False,  include_raw_content: bool =  False,):):  \"\"\"Run a web search\"\"\"  \"\"\"Run a web search\"\"\" return tavily_client.search( return tavily_client.search( query, query, max_results=max_results,  max_results =max_results, include_raw_content=include_raw_content,  include_raw_content =include_raw_content, topic=topic,  topic =topic, ) )\n```\n\n### [â€‹](#step-4:-create-a-deep-agent) Step 4: Create a deep agent\n\nCopy\n\n```\n# System prompt to steer the agent to be an expert researcher # System prompt to steer the agent to be an expert researcherresearch_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report. research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report. You have access to an internet search tool as your primary means of gathering information.You have access to an internet search tool as your primary means of gathering information. ## `internet_search` ## `internet_search` Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included. \"\"\" \"\"\" agent = create_deep_agent(agent = create_deep_agent( tools=[internet_search],  tools =[internet_search], system_prompt=research_instructions  system_prompt = research_instructions))\n```\n\n### [â€‹](#step-5:-run-the-agent) Step 5: Run the agent\n\nCopy\n\n```\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]}) result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]}) # Print the agent's response # Print the agent's responseprint(result[\"messages\"][-1].content) print(result[\"messages\"][- 1].content)\n```\n\n## [â€‹](#what-happened) What happened?\n\nYour deep agent automatically:\n\n1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task\n2. **Conducted research**: Called the `internet_search` tool to gather information\n3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results\n4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents\n5. **Synthesized a report**: Compiled findings into a coherent response\n\n## [â€‹](#next-steps) Next steps\n\nNow that youâ€™ve built your first deep agent:\n\n* **Customize your agent**: Learn about [customization options](/oss/python/deepagents/customization), including custom system prompts, tools, and subagents.\n* **Understand middleware**: Dive into the [middleware architecture](/oss/python/deepagents/middleware) that powers deep agents.\n* **Add long-term memory**: Enable [persistent memory](/oss/python/deepagents/long-term-memory) across conversations.\n* **Deploy to production**: Learn about [deployment options](/oss/python/langgraph/deploy) for LangGraph applications.\n\n---\n\n[Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n\n[Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\n[Deep Agents overview\n\nPrevious](/oss/python/deepagents/overview)[Customize Deep Agents](/oss/python/deepagents/customization)"}, {"url": "https://github.com/langchain-ai/deepagents", "title": "langchain-ai/deepagents: Deep Agents is an agent harness ... - GitHub", "content": "import os from deepagents import create_deep_agent from tavily import TavilyClient tavily_client = TavilyClient api_key = os environ \"TAVILY_API_KEY\" def internet_search query str max_results int = 5 \"\"\"Run a web search\"\"\" return tavily_client search query max_results = max_results agent = create_deep_agent tools = internet_search system_prompt =\"Conduct research and write a polished report.\" result = agent invoke \"messages\" \"role\" \"user\" \"content\" \"What is LangGraph?\". Deep agents use middleware for extensibility (see Built-in Tools for defaults). from deepagents import create_deep_agent research_subagent = \"name\"\"research-agent\" \"description\"\"Used to research in-depth questions\" \"system_prompt\" \"You are an expert researcher\" \"tools\" internet_search \"model\"\"openai:gpt-4o\"# Optional, defaults to main agent model agent = create_deep_agent subagents = research_subagent. from deepagents import CompiledSubAgent create_deep_agent custom_graph = create_agent model = tools = system_prompt = agent = create_deep_agent subagents = CompiledSubAgent name =\"data-analyzer\" description = \"Specialized agent for data analysis\" runnable = custom_graph. from langchain_core tools import tool from deepagents import create_deep_agent @tool tool def get_weather city str -> str\"\"\"Get the weather in a city.\"\"\" returnf\"The weather in {city} is sunny.\"{city}{city} agent = create_deep_agent model =\"anthropic:claude-sonnet-4-20250514\" tools = get_weather interrupt_on = \"get_weather\" \"allowed_decisions\" \"approve\" \"edit\" \"reject\".", "score": 0.7738498, "raw_content": "[Skip to content](#start-of-content)   \n\n\n\n## Navigation Menu\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Fdeepagents) \n\nAppearance settings\n\n# Search code, repositories, users, issues, pull requests...\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Fdeepagents)\n\n [Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=langchain-ai%2Fdeepagents) \n\nAppearance settings\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\n\n{{ message }}\n\n[langchain-ai](/langchain-ai)   /  **[deepagents](/langchain-ai/deepagents)**  Public\n\n* [Notifications](/login?return_to=%2Flangchain-ai%2Fdeepagents)  You must be signed in to change notification settings\n* [Fork 1.2k](/login?return_to=%2Flangchain-ai%2Fdeepagents)\n* [Star  7.8k](/login?return_to=%2Flangchain-ai%2Fdeepagents)\n\nDeep Agents is an agent harness built on langchain and langgraph. Deep Agents are equipped with a planning tool, a filesystem backend, and the ability to spawn subagents - making them well-equipped to handle complex agentic tasks.\n\n[docs.langchain.com/oss/python/deepagents/overview](https://docs.langchain.com/oss/python/deepagents/overview \"https://docs.langchain.com/oss/python/deepagents/overview\")\n\n### License\n\n[MIT license](/langchain-ai/deepagents/blob/master/LICENSE)\n\n[7.8k stars](/langchain-ai/deepagents/stargazers)   [1.2k forks](/langchain-ai/deepagents/forks)   [Branches](/langchain-ai/deepagents/branches)   [Tags](/langchain-ai/deepagents/tags)   [Activity](/langchain-ai/deepagents/activity)\n\n[Star](/login?return_to=%2Flangchain-ai%2Fdeepagents)\n\n[Notifications](/login?return_to=%2Flangchain-ai%2Fdeepagents)  You must be signed in to change notification settings\n\n# langchain-ai/deepagents\n\n[Branches](/langchain-ai/deepagents/branches)[Tags](/langchain-ai/deepagents/tags)\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit   History[225 Commits](/langchain-ai/deepagents/commits/master/) |\n| [.github](/langchain-ai/deepagents/tree/master/.github \".github\") | [.github](/langchain-ai/deepagents/tree/master/.github \".github\") |  |  |\n| [libs](/langchain-ai/deepagents/tree/master/libs \"libs\") | [libs](/langchain-ai/deepagents/tree/master/libs \"libs\") |  |  |\n| [.gitignore](/langchain-ai/deepagents/blob/master/.gitignore \".gitignore\") | [.gitignore](/langchain-ai/deepagents/blob/master/.gitignore \".gitignore\") |  |  |\n| [LICENSE](/langchain-ai/deepagents/blob/master/LICENSE \"LICENSE\") | [LICENSE](/langchain-ai/deepagents/blob/master/LICENSE \"LICENSE\") |  |  |\n| [README.md](/langchain-ai/deepagents/blob/master/README.md \"README.md\") | [README.md](/langchain-ai/deepagents/blob/master/README.md \"README.md\") |  |  |\n|  |\n\n## Repository files navigation\n\n# ðŸš€ðŸ§  Deep Agents\n\nAgents can increasingly tackle long-horizon tasks, [with agent task length doubling every 7 months](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/)! But, long horizon tasks often span dozens of tool calls, which present cost and reliability challenges. Popular agents such as [Claude Code](https://code.claude.com/docs) and [Manus](https://www.youtube.com/watch?v=6_BcCthVvb8) use some common principles to address these challenges, including **planning** (prior to task execution), **computer access** (giving the agent access to a shell and a filesystem), and **sub-agent delegation** (isolated task execution). `deepagents` is a simple agent harness that implements these tools, but is open source and easily extendable with your own custom tools and instructions.\n\n## ðŸ“š Resources\n\n* **[Documentation](https://docs.langchain.com/oss/python/deepagents/overview)** - Full overview and API reference\n* **[Quickstarts Repo](https://github.com/langchain-ai/deepagents-quickstarts)** - Examples and use-cases\n* **[CLI](/langchain-ai/deepagents/blob/master/libs/deepagents-cli)** - Interactive command-line interface with skills, memory, and HITL workflows\n\n## ðŸš€ Quickstart\n\nYou can give `deepagents` custom tools. Below, we'll optionally provide the `tavily` tool to search the web. This tool will be added to the `deepagents` build-in tools (see below).\n\n```\npip install deepagents tavily-python\n```\n\nSet `TAVILY_API_KEY` in your environment ([get one here](https://www.tavily.com/)):\n\n```\nimport os from deepagents import create_deep_agent from tavily import TavilyClient tavily_client = TavilyClient api_key = os environ \"TAVILY_API_KEY\" def internet_search query str max_results int = 5 \"\"\"Run a web search\"\"\" return tavily_client search query max_results = max_results agent = create_deep_agent tools = internet_search system_prompt =\"Conduct research and write a polished report.\" result = agent invoke \"messages\" \"role\" \"user\" \"content\" \"What is LangGraph?\"\n```\n\nThe agent created with `create_deep_agent` is compiled [LangGraph StateGraph](https://docs.langchain.com/oss/python/langgraph/overview), so it can be used with streaming, human-in-the-loop, memory, or Studio just like any LangGraph agent. See our [quickstarts repo](https://github.com/langchain-ai/deepagents-quickstarts) for more examples.\n\n## Customizing Deep Agents\n\nThere are several parameters you can pass to `create_deep_agent`.\n\n### `model`\n\nBy default, `deepagents` uses `\"claude-sonnet-4-5-20250929\"`. You can customize this by passing any [LangChain model object](https://python.langchain.com/docs/integrations/chat/).\n\n```\nfrom langchain chat_models import init_chat_model from deepagents import create_deep_agent model = init_chat_model\"openai:gpt-4o\" agent = create_deep_agent model = model\n```\n\n### `system_prompt`\n\nYou can provide a `system_prompt` parameter to `create_deep_agent()`. This custom prompt is **appended to** default instructions that are automatically injected by middleware.\n\nWhen writing a custom system prompt, you should:\n\n* âœ… Define domain-specific workflows (e.g., research methodology, data analysis steps)\n* âœ… Provide concrete examples for your use case\n* âœ… Add specialized guidance (e.g., \"batch similar research tasks into a single TODO\")\n* âœ… Define stopping criteria and resource limits\n* âœ… Explain how tools work together in your workflow\n\n**Don't:**\n\n* âŒ Re-explain what standard tools do (already covered by middleware)\n* âŒ Duplicate middleware instructions about tool usage\n* âŒ Contradict default instructions (work with them, not against them)\n\n```\nfrom deepagents import create_deep_agent research_instructions = \"\"\"your custom system prompt\"\"\" agent = create_deep_agent system_prompt = research_instructions\n```\n\nSee our [quickstarts repo](https://github.com/langchain-ai/deepagents-quickstarts) for more examples.\n\n### `tools`\n\nProvide custom tools to your agent (in addition to [Built-in Tools](#built-in-tools)):\n\n```\nfrom deepagents import create_deep_agent def internet_search query str -> str \"\"\"Run a web search\"\"\" return tavily_client search query agent = create_deep_agent tools = internet_search\n```\n\nYou can also connect MCP tools via [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters):\n\n```\nfrom langchain_mcp_adapters client import MultiServerMCPClient from deepagents import create_deep_agent async def main mcp_client = MultiServerMCPClient mcp_tools = await mcp_client get_tools agent = create_deep_agent tools = mcp_tools async for chunk in agent astream \"messages\" \"role\" \"user\" \"content\"\"...\" chunk \"messages\" - 1 pretty_print\n```\n\n### `middleware`\n\nDeep agents use [middleware](https://docs.langchain.com/oss/python/langchain/middleware) for extensibility (see [Built-in Tools](#built-in-tools) for defaults). Add custom middleware to inject tools, modify prompts, or hook into the agent lifecycle:\n\n```\nfrom langchain_core tools import tool from deepagents import create_deep_agent from langchain agents middleware import AgentMiddleware @tool tool def get_weather city str -> str\"\"\"Get the weather in a city.\"\"\" returnf\"The weather in {city} is sunny.\"{city}{city} class WeatherMiddleware AgentMiddleware tools = get_weather agent = create_deep_agent middleware = WeatherMiddleware\n```\n\n### `subagents`\n\nThe main agent can delegate work to sub-agents via the `task` tool (see [Built-in Tools](#built-in-tools)). You can supply custom sub-agents for context isolation and custom instructions:\n\n```\nfrom deepagents import create_deep_agent research_subagent = \"name\"\"research-agent\" \"description\"\"Used to research in-depth questions\" \"system_prompt\" \"You are an expert researcher\" \"tools\" internet_search \"model\"\"openai:gpt-4o\"# Optional, defaults to main agent model agent = create_deep_agent subagents = research_subagent\n```\n\nFor complex cases, pass a pre-built LangGraph graph:\n\n```\nfrom deepagents import CompiledSubAgent create_deep_agent custom_graph = create_agent model = tools = system_prompt = agent = create_deep_agent subagents = CompiledSubAgent name =\"data-analyzer\" description = \"Specialized agent for data analysis\" runnable = custom_graph\n```\n\nSee the [subagents documentation](https://docs.langchain.com/oss/python/deepagents/subagents) for more details.\n\n### `interrupt_on`\n\nSome tools may be sensitive and require human approval before execution. Deepagents supports human-in-the-loop workflows through LangGraphâ€™s interrupt capabilities. You can configure which tools require approval using a checkpointer.\n\nThese tool configs are passed to our prebuilt [HITL middleware](https://docs.langchain.com/oss/python/langchain/middleware#human-in-the-loop) so that the agent pauses execution and waits for feedback from the user before executing configured tools.\n\n```\nfrom langchain_core tools import tool from deepagents import create_deep_agent @tool tool def get_weather city str -> str\"\"\"Get the weather in a city.\"\"\" returnf\"The weather in {city} is sunny.\"{city}{city} agent = create_deep_agent model =\"anthropic:claude-sonnet-4-20250514\" tools = get_weather interrupt_on = \"get_weather\" \"allowed_decisions\" \"approve\" \"edit\" \"reject\"\n```\n\nSee the [human-in-the-loop documentation](https://docs.langchain.com/oss/python/deepagents/human-in-the-loop) for more details.\n\n### `backend`\n\nDeep agents use pluggable backends to control how filesystem operations work. By default, files are stored in the agent's ephemeral state. You can configure different backends for local disk access, persistent cross-conversation storage, or hybrid routing.\n\n```\nfrom deepagents import create_deep_agent from deepagents backends import FilesystemBackend agent = create_deep_agent backend = FilesystemBackend root_dir =\"/path/to/project\"\n```\n\nAvailable backends include:\n\n* **StateBackend** (default): Ephemeral files stored in agent state\n* **FilesystemBackend**: Real disk operations under a root directory\n* **StoreBackend**: Persistent storage using LangGraph Store\n* **CompositeBackend**: Route different paths to different backends\n\nSee the [backends documentation](https://docs.langchain.com/oss/python/deepagents/backends) for more details.\n\n### Long-term Memory\n\nDeep agents can maintain persistent memory across conversations using a `CompositeBackend` that routes specific paths to durable storage.\n\nThis enables hybrid memory where working files remain ephemeral while important data (like user preferences or knowledge bases) persists across threads.\n\n```\nfrom deepagents import create_deep_agent from deepagents backends import CompositeBackend StateBackend StoreBackend from langgraph store memory import InMemoryStore agent = create_deep_agent backend = CompositeBackend default = StateBackend routes =\"/memories/\" StoreBackend store = InMemoryStore\n```\n\nFiles under `/memories/` will persist across all conversations, while other paths remain temporary. Use cases include:\n\n* Preserving user preferences across sessions\n* Building knowledge bases from multiple conversations\n* Self-improving instructions based on feedback\n* Maintaining research progress across sessions\n\nSee the [long-term memory documentation](https://docs.langchain.com/oss/python/deepagents/long-term-memory) for more details.\n\n## Built-in Tools\n\nEvery deep agent created with `create_deep_agent` comes with a standard set of tools:\n\n| Tool Name | Description | Provided By |\n| --- | --- | --- |\n| `write_todos` | Create and manage structured task lists for tracking progress through complex workflows | TodoListMiddleware |\n| `read_todos` | Read the current todo list state | TodoListMiddleware |\n| `ls` | List all files in a directory (requires absolute path) | FilesystemMiddleware |\n| `read_file` | Read content from a file with optional pagination (offset/limit parameters) | FilesystemMiddleware |\n| `write_file` | Create a new file or completely overwrite an existing file | FilesystemMiddleware |\n| `edit_file` | Perform exact string replacements in files | FilesystemMiddleware |\n| `glob` | Find files matching a pattern (e.g., `**/*.py`) | FilesystemMiddleware |\n| `grep` | Search for text patterns within files | FilesystemMiddleware |\n| `execute`\\* | Run shell commands in a sandboxed environment | FilesystemMiddleware |\n| `task` | Delegate tasks to specialized sub-agents with isolated context windows | SubAgentMiddleware |\n\nThe `execute` tool is only available if the backend implements `SandboxBackendProtocol`. By default, it uses the in-memory state backend which does not support command execution. As shown, these tools (along with other capabilities) are provided by default middleware:\n\nSee the [agent harness documentation](https://docs.langchain.com/oss/python/deepagents/harness) for more details on built-in tools and capabilities.\n\n## Built-in Middleware\n\n`deepagents` uses middleware under the hood. Here is the list of the middleware used.\n\n| Middleware | Purpose |\n| --- | --- |\n| **TodoListMiddleware** | Task planning and progress tracking |\n| **FilesystemMiddleware** | File operations and context offloading (auto-saves large results) |\n| **SubAgentMiddleware** | Delegate tasks to isolated sub-agents |\n| **SummarizationMiddleware** | Auto-summarizes when context exceeds 170k tokens |\n| **AnthropicPromptCachingMiddleware** | Caches system prompts to reduce costs (Anthropic only) |\n| **PatchToolCallsMiddleware** | Fixes dangling tool calls from interruptions |\n| **HumanInTheLoopMiddleware** | Pauses execution for human approval (requires `interrupt_on` config) |\n\n## Built-in prompts\n\nThe middleware automatically adds instructions about the standard tools. Your custom instructions should **complement, not duplicate** these defaults:\n\n#### From [TodoListMiddleware](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/middleware/todo.py)\n\n* Explains when to use `write_todos` and `read_todos`\n* Guidance on marking tasks completed\n* Best practices for todo list management\n* When NOT to use todos (simple tasks)\n\n#### From [FilesystemMiddleware](/langchain-ai/deepagents/blob/master/libs/deepagents/deepagents/middleware/filesystem.py)\n\n* Lists all filesystem tools (`ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep`, `execute`\\*)\n* Explains that file paths must start with `/`\n* Describes each tool's purpose and parameters\n* Notes about context offloading for large tool results\n\n#### From [SubAgentMiddleware](/langchain-ai/deepagents/blob/master/libs/deepagents/deepagents/middleware/subagents.py)\n\n* Explains the `task()` tool for delegating to sub-agents\n* When to use sub-agents vs when NOT to use them\n* Guidance on parallel execution\n* Subagent lifecycle (spawn â†’ run â†’ return â†’ reconcile)\n\n## Security Considerations\n\n### Trust Model\n\nDeepagents follows a \"trust the LLM\" model similar to Claude Code. The agent can perform any action the underlying tools allow. Security boundaries should be enforced at the tool/sandbox level, not by expecting the LLM to self-police.\n\n## About\n\nDeep Agents is an agent harness built on langchain and langgraph. Deep Agents are equipped with a planning tool, a filesystem backend, and the ability to spawn subagents - making them well-equipped to handle complex agentic tasks.\n\n[docs.langchain.com/oss/python/deepagents/overview](https://docs.langchain.com/oss/python/deepagents/overview \"https://docs.langchain.com/oss/python/deepagents/overview\")\n\n### Topics\n\n[agents](/topics/agents \"Topic: agents\")   [langchain](/topics/langchain \"Topic: langchain\")   [langgraph](/topics/langgraph \"Topic: langgraph\")   [deepagents](/topics/deepagents \"Topic: deepagents\")\n\n### Resources\n\n### License\n\n[MIT license](#MIT-1-ov-file)\n\n### Code of conduct\n\n[Code of conduct](#coc-ov-file)\n\n### Contributing\n\n### Security policy\n\n[Security policy](#security-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Custom properties](/langchain-ai/deepagents/custom-properties)\n\n### Stars\n\n[**7.8k** stars](/langchain-ai/deepagents/stargazers)\n\n### Watchers\n\n[**65** watching](/langchain-ai/deepagents/watchers)\n\n### Forks\n\n[**1.2k** forks](/langchain-ai/deepagents/forks)\n\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Fdeepagents&report=langchain-ai+%28user%29)\n\n## [Releases 11](/langchain-ai/deepagents/releases)\n\n[deepagents==0.3.1  Latest\n\nDec 23, 2025](/langchain-ai/deepagents/releases/tag/deepagents%3D%3D0.3.1)\n\n[+ 10 releases](/langchain-ai/deepagents/releases)\n\n## [Packages 0](/orgs/langchain-ai/packages?repo_name=deepagents)\n\nNo packages published\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## [Contributors 37](/langchain-ai/deepagents/graphs/contributors)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[+ 23 contributors](/langchain-ai/deepagents/graphs/contributors)\n\n## Languages\n\n* [Python 99.3%](/langchain-ai/deepagents/search?l=python)\n* [Makefile 0.7%](/langchain-ai/deepagents/search?l=makefile)\n\nYou canâ€™t perform that action at this time.\n\n "}, {"url": "https://reference.langchain.com/python/deepagents/", "title": "Deep Agents reference - LangChain Docs", "content": "create_deep_agent( create_deep_agent( model: str | model: str | BaseChatModel | None = None, tools: Sequence[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool | Callable | dict[str, Any]] | None = None, tools: Sequence[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool BaseTool | Callable | dict[str, Any]] | None = None, *, *, system_prompt: str | None = None, system_prompt: str | None = None, middleware: Sequence[            AgentMiddleware (langchain.agents.middleware.types.AgentMiddleware)\">AgentMiddleware] = (), middleware: Sequence[            AgentMiddleware (langchain.agents.middleware.types.AgentMiddleware)\">AgentMiddleware AgentMiddleware] =(), subagents: list[            SubAgent (deepagents.middleware.subagents.SubAgent)\">SubAgent |             CompiledSubAgent (deepagents.middleware.subagents.CompiledSubAgent)\">CompiledSubAgent] | None = None, subagents: list[            SubAgent (deepagents.middleware.subagents.SubAgent)\">SubAgent SubAgent |            CompiledSubAgent (deepagents.middleware.subagents.CompiledSubAgent)\">CompiledSubAgent CompiledSubAgent] | None = None, response_format: ResponseFormat | None = None, response_format: ResponseFormat ResponseFormat | None = None, context_schema: type[Any] | None = None, context_schema: type[Any] | None = None, checkpointer: Checkpointer | None = None, checkpointer: Checkpointer Checkpointer | None = None, store:             BaseStore (langgraph.store.base.BaseStore)\">BaseStore | None = None, store:            BaseStore (langgraph.store.base.BaseStore)\">BaseStore BaseStore | None = None, backend: BackendProtocol | BackendFactory | None = None, backend: BackendProtocol BackendProtocol | BackendFactory BackendFactory | None = None, interrupt_on: dict[str, bool |             InterruptOnConfig (langchain.agents.middleware.InterruptOnConfig)\">InterruptOnConfig] | None = None, interrupt_on: dict[str, bool |            InterruptOnConfig (langchain.agents.middleware.InterruptOnConfig)\">InterruptOnConfig InterruptOnConfig] | None = None, debug: bool = False, debug: bool = False, name: str | None = None, name: str | None = None, cache:             BaseCache (langgraph.cache.base.BaseCache)\">BaseCache | None = None, cache:            BaseCache (langgraph.cache.base.BaseCache)\">BaseCache BaseCache | None = None,) ->             CompiledStateGraph (langgraph.graph.state.CompiledStateGraph)\">CompiledStateGraph ) ->            CompiledStateGraph (langgraph.graph.state.CompiledStateGraph)\">CompiledStateGraph CompiledStateGraph.", "score": 0.76596165, "raw_content": "[Skip to content](#deepagents)\n\n\n\n* [deepagents](#deepagents) \n\n  + [FilesystemMiddleware](#deepagents.FilesystemMiddleware) \n\n    - [name](#deepagents.FilesystemMiddleware.name)\n    - [state\\_schema](#deepagents.FilesystemMiddleware.state_schema)\n    - [tools](#deepagents.FilesystemMiddleware.tools)\n    - [before\\_agent](#deepagents.FilesystemMiddleware.before_agent)\n    - [abefore\\_agent](#deepagents.FilesystemMiddleware.abefore_agent)\n    - [before\\_model](#deepagents.FilesystemMiddleware.before_model)\n    - [abefore\\_model](#deepagents.FilesystemMiddleware.abefore_model)\n    - [after\\_model](#deepagents.FilesystemMiddleware.after_model)\n    - [aafter\\_model](#deepagents.FilesystemMiddleware.aafter_model)\n    - [after\\_agent](#deepagents.FilesystemMiddleware.after_agent)\n    - [aafter\\_agent](#deepagents.FilesystemMiddleware.aafter_agent)\n    - [\\_\\_init\\_\\_](#deepagents.FilesystemMiddleware.__init__)\n    - [wrap\\_model\\_call](#deepagents.FilesystemMiddleware.wrap_model_call)\n    - [awrap\\_model\\_call](#deepagents.FilesystemMiddleware.awrap_model_call)\n    - [wrap\\_tool\\_call](#deepagents.FilesystemMiddleware.wrap_tool_call)\n    - [awrap\\_tool\\_call](#deepagents.FilesystemMiddleware.awrap_tool_call)\n  + [CompiledSubAgent](#deepagents.CompiledSubAgent) \n\n    - [name](#deepagents.CompiledSubAgent.name)\n    - [description](#deepagents.CompiledSubAgent.description)\n    - [runnable](#deepagents.CompiledSubAgent.runnable)\n  + [SubAgent](#deepagents.SubAgent) \n\n    - [name](#deepagents.SubAgent.name)\n    - [description](#deepagents.SubAgent.description)\n    - [system\\_prompt](#deepagents.SubAgent.system_prompt)\n    - [tools](#deepagents.SubAgent.tools)\n    - [model](#deepagents.SubAgent.model)\n    - [middleware](#deepagents.SubAgent.middleware)\n    - [interrupt\\_on](#deepagents.SubAgent.interrupt_on)\n  + [SubAgentMiddleware](#deepagents.SubAgentMiddleware) \n\n    - [state\\_schema](#deepagents.SubAgentMiddleware.state_schema)\n    - [name](#deepagents.SubAgentMiddleware.name)\n    - [tools](#deepagents.SubAgentMiddleware.tools)\n    - [before\\_agent](#deepagents.SubAgentMiddleware.before_agent)\n    - [abefore\\_agent](#deepagents.SubAgentMiddleware.abefore_agent)\n    - [before\\_model](#deepagents.SubAgentMiddleware.before_model)\n    - [abefore\\_model](#deepagents.SubAgentMiddleware.abefore_model)\n    - [after\\_model](#deepagents.SubAgentMiddleware.after_model)\n    - [aafter\\_model](#deepagents.SubAgentMiddleware.aafter_model)\n    - [after\\_agent](#deepagents.SubAgentMiddleware.after_agent)\n    - [aafter\\_agent](#deepagents.SubAgentMiddleware.aafter_agent)\n    - [wrap\\_tool\\_call](#deepagents.SubAgentMiddleware.wrap_tool_call)\n    - [awrap\\_tool\\_call](#deepagents.SubAgentMiddleware.awrap_tool_call)\n    - [\\_\\_init\\_\\_](#deepagents.SubAgentMiddleware.__init__)\n    - [wrap\\_model\\_call](#deepagents.SubAgentMiddleware.wrap_model_call)\n    - [awrap\\_model\\_call](#deepagents.SubAgentMiddleware.awrap_model_call)\n  + [create\\_deep\\_agent](#deepagents.create_deep_agent)\n\n# Deep Agents reference\n\nWelcome to the [Deep Agents](https://github.com/langchain-ai/deepagents) reference documentation!\n\nWork in progress\n\nThis page is a work in progress, and we appreciate your patience as we continue to expand and improve the content.\n\n## deepagents [Â¶](#deepagents \"Copy anchor link to this section for reference\")\n\nDeepAgents package.\n\n| FUNCTION | DESCRIPTION |\n| --- | --- |\n| create\\_deep\\_agent (`deepagents.create_deep_agent`)\">create\\_deep\\_agent | Create a deep agent. |\n\n### FilesystemMiddleware [Â¶](#deepagents.FilesystemMiddleware \"Copy anchor link to this section for reference\")\n\nBases:  AgentMiddleware (`langchain.agents.middleware.types.AgentMiddleware`)\">AgentMiddleware\n\nMiddleware for providing filesystem and optional execution tools to an agent.\n\nThis middleware adds filesystem tools to the agent: ls, read\\_file, write\\_file, edit\\_file, glob, and grep. Files can be stored using any backend that implements the BackendProtocol.\n\nIf the backend implements SandboxBackendProtocol, an execute tool is also added for running shell commands.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `backend` | Backend for file storage and optional execution. If not provided, defaults to StateBackend (ephemeral storage in agent state). For persistent storage or hybrid setups, use CompositeBackend with custom routes. For execution support, use a backend that implements SandboxBackendProtocol.  **TYPE:** `BACKEND_TYPES`   **DEFAULT:** `None` |\n| `system_prompt` | Optional custom system prompt override.  **TYPE:** `str | None`   **DEFAULT:** `None` |\n| `custom_tool_descriptions` | Optional custom tool descriptions override.  **TYPE:** `dict[str, str] | None`   **DEFAULT:** `None` |\n| `tool_token_limit_before_evict` | Optional token limit before evicting a tool result to the filesystem.  **TYPE:** `int | None`   **DEFAULT:** `20000` |\n\n Example\n\n```\nfrom deepagents.middleware.filesystem import FilesystemMiddleware from  deepagents.middleware.filesystem  import FilesystemMiddlewarefrom deepagents.backends import StateBackend, StoreBackend, CompositeBackend from  deepagents.backends  import StateBackend, StoreBackend, CompositeBackendfrom langchain.agents import create_agent from  langchain.agents  import create_agent  # Ephemeral storage only (default, no execution) # Ephemeral storage only (default, no execution)agent = create_agent(middleware=[FilesystemMiddleware()]) agent = create_agent(middleware =[FilesystemMiddleware()])  # With hybrid storage (ephemeral + persistent /memories/) # With hybrid storage (ephemeral + persistent /memories/)backend = CompositeBackend(default=StateBackend(), routes={\"/memories/\": StoreBackend()}) backend = CompositeBackend(default = StateBackend(), routes ={\"/memories/\": StoreBackend()})agent = create_agent(middleware=[FilesystemMiddleware(backend=backend)]) agent = create_agent(middleware =[FilesystemMiddleware(backend = backend)])  # With sandbox backend (supports execution) # With sandbox backend (supports execution) from my_sandbox import DockerSandboxBackend from  my_sandbox  import DockerSandboxBackend  sandbox = DockerSandboxBackend(container_id=\"my-container\") sandbox = DockerSandboxBackend(container_id =\"my-container\")agent = create_agent(middleware=[FilesystemMiddleware(backend=sandbox)]) agent = create_agent(middleware =[FilesystemMiddleware(backend = sandbox)])\n```\n\n| METHOD | DESCRIPTION |\n| --- | --- |\n| before\\_agent (`deepagents.FilesystemMiddleware.before_agent`)\">before\\_agent | Logic to run before the agent execution starts. |\n| abefore\\_agent  `async`  (`deepagents.FilesystemMiddleware.abefore_agent`)\">abefore\\_agent | Async logic to run before the agent execution starts. |\n| before\\_model (`deepagents.FilesystemMiddleware.before_model`)\">before\\_model | Logic to run before the model is called. |\n| abefore\\_model  `async`  (`deepagents.FilesystemMiddleware.abefore_model`)\">abefore\\_model | Async logic to run before the model is called. |\n| after\\_model (`deepagents.FilesystemMiddleware.after_model`)\">after\\_model | Logic to run after the model is called. |\n| aafter\\_model  `async`  (`deepagents.FilesystemMiddleware.aafter_model`)\">aafter\\_model | Async logic to run after the model is called. |\n| after\\_agent (`deepagents.FilesystemMiddleware.after_agent`)\">after\\_agent | Logic to run after the agent execution completes. |\n| aafter\\_agent  `async`  (`deepagents.FilesystemMiddleware.aafter_agent`)\">aafter\\_agent | Async logic to run after the agent execution completes. |\n| \\_\\_init\\_\\_ (`deepagents.FilesystemMiddleware.__init__`)\">\\_\\_init\\_\\_ | Initialize the filesystem middleware. |\n| wrap\\_model\\_call (`deepagents.FilesystemMiddleware.wrap_model_call`)\">wrap\\_model\\_call | Update the system prompt and filter tools based on backend capabilities. |\n| awrap\\_model\\_call  `async`  (`deepagents.FilesystemMiddleware.awrap_model_call`)\">awrap\\_model\\_call | (async) Update the system prompt and filter tools based on backend capabilities. |\n| wrap\\_tool\\_call (`deepagents.FilesystemMiddleware.wrap_tool_call`)\">wrap\\_tool\\_call | Check the size of the tool call result and evict to filesystem if too large. |\n| awrap\\_tool\\_call  `async`  (`deepagents.FilesystemMiddleware.awrap_tool_call`)\">awrap\\_tool\\_call | (async)Check the size of the tool call result and evict to filesystem if too large. |\n\n#### name `property` [Â¶](#deepagents.FilesystemMiddleware.name \"Copy anchor link to this section for reference\")\n\n```\nname: str name: str\n```\n\nThe name of the middleware instance.\n\nDefaults to the class name, but can be overridden for custom naming.\n\n#### state\\_schema `class-attribute` `instance-attribute` [Â¶](#deepagents.FilesystemMiddleware.state_schema \"Copy anchor link to this section for reference\")\n\n```\nstate_schema = FilesystemState state_schema = FilesystemState FilesystemState\n```\n\nThe schema for state passed to the middleware nodes.\n\n#### tools `instance-attribute` [Â¶](#deepagents.FilesystemMiddleware.tools \"Copy anchor link to this section for reference\")\n\n```\ntools = _get_filesystem_tools(backend, custom_tool_descriptions) tools = _get_filesystem_tools _get_filesystem_tools(backend backend, custom_tool_descriptions custom_tool_descriptions)\n```\n\nAdditional tools registered by the middleware.\n\n#### before\\_agent [Â¶](#deepagents.FilesystemMiddleware.before_agent \"Copy anchor link to this section for reference\")\n\n```\nbefore_agent(state: StateT, runtime: before_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run before the agent execution starts.\n\nAsync version is `abefore_agent`\n\n#### abefore\\_agent `async` [Â¶](#deepagents.FilesystemMiddleware.abefore_agent \"Copy anchor link to this section for reference\")\n\n```\nabefore_agent(state: StateT, runtime: abefore_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run before the agent execution starts.\n\n#### before\\_model [Â¶](#deepagents.FilesystemMiddleware.before_model \"Copy anchor link to this section for reference\")\n\n```\nbefore_model(state: StateT, runtime: before_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run before the model is called.\n\nAsync version is `abefore_model`\n\n#### abefore\\_model `async` [Â¶](#deepagents.FilesystemMiddleware.abefore_model \"Copy anchor link to this section for reference\")\n\n```\nabefore_model(state: StateT, runtime: abefore_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run before the model is called.\n\n#### after\\_model [Â¶](#deepagents.FilesystemMiddleware.after_model \"Copy anchor link to this section for reference\")\n\n```\nafter_model(state: StateT, runtime: after_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run after the model is called.\n\nAsync version is `aafter_model`\n\n#### aafter\\_model `async` [Â¶](#deepagents.FilesystemMiddleware.aafter_model \"Copy anchor link to this section for reference\")\n\n```\naafter_model(state: StateT, runtime: aafter_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run after the model is called.\n\n#### after\\_agent [Â¶](#deepagents.FilesystemMiddleware.after_agent \"Copy anchor link to this section for reference\")\n\n```\nafter_agent(state: StateT, runtime: after_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run after the agent execution completes.\n\n#### aafter\\_agent `async` [Â¶](#deepagents.FilesystemMiddleware.aafter_agent \"Copy anchor link to this section for reference\")\n\n```\naafter_agent(state: StateT, runtime: aafter_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run after the agent execution completes.\n\n#### \\_\\_init\\_\\_ [Â¶](#deepagents.FilesystemMiddleware.__init__ \"Copy anchor link to this section for reference\")\n\n```\n__init__( __init__( *, *, backend: BACKEND_TYPES | None = None, backend: BACKEND_TYPES BACKEND_TYPES | None = None, system_prompt: str | None = None, system_prompt: str | None = None, custom_tool_descriptions: dict[str, str] | None = None, custom_tool_descriptions: dict[str, str] | None = None, tool_token_limit_before_evict: int | None = 20000, tool_token_limit_before_evict: int | None = 20000,) -> None ) -> None\n```\n\nInitialize the filesystem middleware.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `backend` | Backend for file storage and optional execution, or a factory callable. Defaults to StateBackend if not provided.  **TYPE:** `BACKEND_TYPES`   **DEFAULT:** `None` |\n| `system_prompt` | Optional custom system prompt override.  **TYPE:** `str | None`   **DEFAULT:** `None` |\n| `custom_tool_descriptions` | Optional custom tool descriptions override.  **TYPE:** `dict[str, str] | None`   **DEFAULT:** `None` |\n| `tool_token_limit_before_evict` | Optional token limit before evicting a tool result to the filesystem.  **TYPE:** `int | None`   **DEFAULT:** `20000` |\n\n#### wrap\\_model\\_call [Â¶](#deepagents.FilesystemMiddleware.wrap_model_call \"Copy anchor link to this section for reference\")\n\n```\nwrap_model_call( wrap_model_call( request: request:             ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest, handler: Callable[[            ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest],             ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse] ) ->             ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse \n```\n\nUpdate the system prompt and filter tools based on backend capabilities.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `request` | The model request being processed.  **TYPE:**  ModelRequest  `dataclass`  (`langchain.agents.middleware.types.ModelRequest`)\">ModelRequest |\n| `handler` | The handler function to call with the modified request.  **TYPE:** `Callable[[` ModelRequest  `dataclass`  (`langchain.agents.middleware.types.ModelRequest`)\">ModelRequest],  [ModelResponse  `dataclass`  (`langchain.agents.middleware.types.ModelResponse`)\">ModelResponse](../langchain/middleware/#langchain.agents.middleware.ModelResponse \"<code class=\")] |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| ModelResponse  `dataclass`  (`langchain.agents.middleware.types.ModelResponse`)\">ModelResponse | The model response from the handler. |\n\n#### awrap\\_model\\_call `async` [Â¶](#deepagents.FilesystemMiddleware.awrap_model_call \"Copy anchor link to this section for reference\")\n\n```\nawrap_model_call( awrap_model_call( request: request:             ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest, handler: Callable[[            ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest], Awaitable[            ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse]] ) ->             ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse \n```\n\n(async) Update the system prompt and filter tools based on backend capabilities.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `request` | The model request being processed.  **TYPE:**  ModelRequest  `dataclass`  (`langchain.agents.middleware.types.ModelRequest`)\">ModelRequest |\n| `handler` | The handler function to call with the modified request.  **TYPE:** `Callable[[` ModelRequest  `dataclass`  (`langchain.agents.middleware.types.ModelRequest`)\">ModelRequest], [Awaitable](https://docs.python.org/3/library/collections.abc.html#collections.abc.Awaitable \"<code>collections.abc.Awaitable</code>\")[ [ModelResponse  `dataclass`  (`langchain.agents.middleware.types.ModelResponse`)\">ModelResponse](../langchain/middleware/#langchain.agents.middleware.ModelResponse \"<code class=\")]] |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| ModelResponse  `dataclass`  (`langchain.agents.middleware.types.ModelResponse`)\">ModelResponse | The model response from the handler. |\n\n#### wrap\\_tool\\_call [Â¶](#deepagents.FilesystemMiddleware.wrap_tool_call \"Copy anchor link to this section for reference\")\n\n```\nwrap_tool_call( wrap_tool_call( request: ToolCallRequest, request: ToolCallRequest ToolCallRequest, handler: Callable[[ToolCallRequest], handler: Callable[[ToolCallRequest ToolCallRequest],             ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command], ) ->             ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command \n```\n\nCheck the size of the tool call result and evict to filesystem if too large.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `request` | The tool call request being processed.  **TYPE:** `ToolCallRequest` |\n| `handler` | The handler function to call with the modified request.  **TYPE:** `ToolCallRequest` ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\")] |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\") | The raw ToolMessage, or a pseudo tool message with the ToolResult in state. |\n\n#### awrap\\_tool\\_call `async` [Â¶](#deepagents.FilesystemMiddleware.awrap_tool_call \"Copy anchor link to this section for reference\")\n\n```\nawrap_tool_call( awrap_tool_call( request: ToolCallRequest, request: ToolCallRequest ToolCallRequest, handler: Callable[[ToolCallRequest], Awaitable[handler: Callable[[ToolCallRequest ToolCallRequest], Awaitable[            ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command]], ) ->             ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command \n```\n\n(async)Check the size of the tool call result and evict to filesystem if too large.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `request` | The tool call request being processed.  **TYPE:** `ToolCallRequest` |\n| `handler` | The handler function to call with the modified request.  **TYPE:** `ToolCallRequest` ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\")]] |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\") | The raw ToolMessage, or a pseudo tool message with the ToolResult in state. |\n\n### CompiledSubAgent [Â¶](#deepagents.CompiledSubAgent \"Copy anchor link to this section for reference\")\n\nBases: `TypedDict`\n\nA pre-compiled agent spec.\n\n#### name `instance-attribute` [Â¶](#deepagents.CompiledSubAgent.name \"Copy anchor link to this section for reference\")\n\n```\nname: str name: str\n```\n\nThe name of the agent.\n\n#### description `instance-attribute` [Â¶](#deepagents.CompiledSubAgent.description \"Copy anchor link to this section for reference\")\n\n```\ndescription: str description: str\n```\n\nThe description of the agent.\n\n#### runnable `instance-attribute` [Â¶](#deepagents.CompiledSubAgent.runnable \"Copy anchor link to this section for reference\")\n\n```\nrunnable: runnable: Runnable\n```\n\nThe Runnable to use for the agent.\n\n### SubAgent [Â¶](#deepagents.SubAgent \"Copy anchor link to this section for reference\")\n\nBases: `TypedDict`\n\nSpecification for an agent.\n\nWhen specifying custom agents, the `default_middleware` from `SubAgentMiddleware` will be applied first, followed by any `middleware` specified in this spec. To use only custom middleware without the defaults, pass `default_middleware=[]` to `SubAgentMiddleware`.\n\n#### name `instance-attribute` [Â¶](#deepagents.SubAgent.name \"Copy anchor link to this section for reference\")\n\n```\nname: str name: str\n```\n\nThe name of the agent.\n\n#### description `instance-attribute` [Â¶](#deepagents.SubAgent.description \"Copy anchor link to this section for reference\")\n\n```\ndescription: str description: str\n```\n\nThe description of the agent.\n\n#### system\\_prompt `instance-attribute` [Â¶](#deepagents.SubAgent.system_prompt \"Copy anchor link to this section for reference\")\n\n```\nsystem_prompt: str system_prompt: str\n```\n\nThe system prompt to use for the agent.\n\n#### tools `instance-attribute` [Â¶](#deepagents.SubAgent.tools \"Copy anchor link to this section for reference\")\n\n```\ntools: Sequence[tools: Sequence[ BaseTool | Callable | dict[str, Any]]\n```\n\nThe tools to use for the agent.\n\n#### model `instance-attribute` [Â¶](#deepagents.SubAgent.model \"Copy anchor link to this section for reference\")\n\n```\nmodel: NotRequired[str | model: NotRequired[str | BaseChatModel]\n```\n\nThe model for the agent. Defaults to `default_model`.\n\n#### middleware `instance-attribute` [Â¶](#deepagents.SubAgent.middleware \"Copy anchor link to this section for reference\")\n\n```\nmiddleware: NotRequired[list[middleware: NotRequired[list[ AgentMiddleware]]\n```\n\nAdditional middleware to append after `default_middleware`.\n\n#### interrupt\\_on `instance-attribute` [Â¶](#deepagents.SubAgent.interrupt_on \"Copy anchor link to this section for reference\")\n\n```\ninterrupt_on: NotRequired[dict[str, bool | interrupt_on: NotRequired[dict[str, bool | InterruptOnConfig]]\n```\n\nThe tool configs to use for the agent.\n\n### SubAgentMiddleware [Â¶](#deepagents.SubAgentMiddleware \"Copy anchor link to this section for reference\")\n\nBases:  AgentMiddleware (`langchain.agents.middleware.types.AgentMiddleware`)\">AgentMiddleware\n\nMiddleware for providing subagents to an agent via a `task` tool.\n\nThis middleware adds a `task` tool to the agent that can be used to invoke subagents. Subagents are useful for handling complex tasks that require multiple steps, or tasks that require a lot of context to resolve.\n\nA chief benefit of subagents is that they can handle multi-step tasks, and then return a clean, concise response to the main agent.\n\nSubagents are also great for different domains of expertise that require a narrower subset of tools and focus.\n\nThis middleware comes with a default general-purpose subagent that can be used to handle the same tasks as the main agent, but with isolated context.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `default_model` | The model to use for subagents. Can be a LanguageModelLike or a dict for init\\_chat\\_model.  **TYPE:** `str |`  BaseChatModel (`langchain_core.language_models.BaseChatModel`)\">BaseChatModel |\n| `default_tools` | The tools to use for the default general-purpose subagent.  **TYPE:** `Sequence[` BaseTool (`langchain.tools.BaseTool`)\">BaseTool | [Callable](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable \"<code>collections.abc.Callable</code>\") | [dict](https://docs.python.org/3/library/stdtypes.html#dict)[[str](https://docs.python.org/3/library/stdtypes.html#str), [Any](https://docs.python.org/3/library/typing.html#typing.Any \"<code>typing.Any</code>\")]] | None   **DEFAULT:** `None` |\n| `default_middleware` | Default middleware to apply to all subagents. If `None` (default), no default middleware is applied. Pass a list to specify custom middleware.  **TYPE:** `list[` AgentMiddleware (`langchain.agents.middleware.types.AgentMiddleware`)\">AgentMiddleware] | None   **DEFAULT:** `None` |\n| `default_interrupt_on` | The tool configs to use for the default general-purpose subagent. These are also the fallback for any subagents that don't specify their own tool configs.  **TYPE:** `dict[str, bool |`  InterruptOnConfig (`langchain.agents.middleware.InterruptOnConfig`)\">InterruptOnConfig] | None   **DEFAULT:** `None` |\n| `subagents` | A list of additional subagents to provide to the agent.  **TYPE:** `list[` SubAgent (`deepagents.middleware.subagents.SubAgent`)\">SubAgent |  [CompiledSubAgent (`deepagents.middleware.subagents.CompiledSubAgent`)\">CompiledSubAgent](#deepagents.CompiledSubAgent \"<code class=\")] | None   **DEFAULT:** `None` |\n| `system_prompt` | Full system prompt override. When provided, completely replaces the agent's system prompt.  **TYPE:** `str | None`   **DEFAULT:** `TASK_SYSTEM_PROMPT` |\n| `general_purpose_agent` | Whether to include the general-purpose agent. Defaults to `True`.  **TYPE:** `bool`   **DEFAULT:** `True` |\n| `task_description` | Custom description for the task tool. If `None`, uses the default description template.  **TYPE:** `str | None`   **DEFAULT:** `None` |\n\n Example\n\n```\nfrom langchain.agents.middleware.subagents import SubAgentMiddleware from  langchain.agents.middleware.subagents  import SubAgentMiddlewarefrom langchain.agents import create_agent from  langchain.agents  import create_agent  # Basic usage with defaults (no default middleware) # Basic usage with defaults (no default middleware)agent = create_agent( agent = create_agent( \"openai:gpt-4o\", \"openai:gpt-4o\", middleware=[ middleware =[ SubAgentMiddleware( SubAgentMiddleware( default_model=\"openai:gpt-4o\", default_model =\"openai:gpt-4o\", subagents=[], subagents =[], ) ) ], ],) )  # Add custom middleware to subagents # Add custom middleware to subagentsagent = create_agent( agent = create_agent( \"openai:gpt-4o\", \"openai:gpt-4o\", middleware=[ middleware =[ SubAgentMiddleware( SubAgentMiddleware( default_model=\"openai:gpt-4o\", default_model =\"openai:gpt-4o\", default_middleware=[TodoListMiddleware()], default_middleware =[TodoListMiddleware()], subagents=[], subagents =[], ) ) ], ],) )\n```\n\n| METHOD | DESCRIPTION |\n| --- | --- |\n| before\\_agent (`deepagents.SubAgentMiddleware.before_agent`)\">before\\_agent | Logic to run before the agent execution starts. |\n| abefore\\_agent  `async`  (`deepagents.SubAgentMiddleware.abefore_agent`)\">abefore\\_agent | Async logic to run before the agent execution starts. |\n| before\\_model (`deepagents.SubAgentMiddleware.before_model`)\">before\\_model | Logic to run before the model is called. |\n| abefore\\_model  `async`  (`deepagents.SubAgentMiddleware.abefore_model`)\">abefore\\_model | Async logic to run before the model is called. |\n| after\\_model (`deepagents.SubAgentMiddleware.after_model`)\">after\\_model | Logic to run after the model is called. |\n| aafter\\_model  `async`  (`deepagents.SubAgentMiddleware.aafter_model`)\">aafter\\_model | Async logic to run after the model is called. |\n| after\\_agent (`deepagents.SubAgentMiddleware.after_agent`)\">after\\_agent | Logic to run after the agent execution completes. |\n| aafter\\_agent  `async`  (`deepagents.SubAgentMiddleware.aafter_agent`)\">aafter\\_agent | Async logic to run after the agent execution completes. |\n| wrap\\_tool\\_call (`deepagents.SubAgentMiddleware.wrap_tool_call`)\">wrap\\_tool\\_call | Intercept tool execution for retries, monitoring, or modification. |\n| awrap\\_tool\\_call  `async`  (`deepagents.SubAgentMiddleware.awrap_tool_call`)\">awrap\\_tool\\_call | Intercept and control async tool execution via handler callback. |\n| \\_\\_init\\_\\_ (`deepagents.SubAgentMiddleware.__init__`)\">\\_\\_init\\_\\_ | Initialize the SubAgentMiddleware. |\n| wrap\\_model\\_call (`deepagents.SubAgentMiddleware.wrap_model_call`)\">wrap\\_model\\_call | Update the system prompt to include instructions on using subagents. |\n| awrap\\_model\\_call  `async`  (`deepagents.SubAgentMiddleware.awrap_model_call`)\">awrap\\_model\\_call | (async) Update the system prompt to include instructions on using subagents. |\n\n#### state\\_schema `class-attribute` `instance-attribute` [Â¶](#deepagents.SubAgentMiddleware.state_schema \"Copy anchor link to this section for reference\")\n\n```\nstate_schema: type[StateT] = cast('type[StateT]', state_schema: type[StateT StateT] = cast('type[StateT]', AgentState)\n```\n\nThe schema for state passed to the middleware nodes.\n\n#### name `property` [Â¶](#deepagents.SubAgentMiddleware.name \"Copy anchor link to this section for reference\")\n\n```\nname: str name: str\n```\n\nThe name of the middleware instance.\n\nDefaults to the class name, but can be overridden for custom naming.\n\n#### tools `instance-attribute` [Â¶](#deepagents.SubAgentMiddleware.tools \"Copy anchor link to this section for reference\")\n\n```\ntools = [task_tool] tools =[task_tool task_tool]\n```\n\nAdditional tools registered by the middleware.\n\n#### before\\_agent [Â¶](#deepagents.SubAgentMiddleware.before_agent \"Copy anchor link to this section for reference\")\n\n```\nbefore_agent(state: StateT, runtime: before_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run before the agent execution starts.\n\nAsync version is `abefore_agent`\n\n#### abefore\\_agent `async` [Â¶](#deepagents.SubAgentMiddleware.abefore_agent \"Copy anchor link to this section for reference\")\n\n```\nabefore_agent(state: StateT, runtime: abefore_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run before the agent execution starts.\n\n#### before\\_model [Â¶](#deepagents.SubAgentMiddleware.before_model \"Copy anchor link to this section for reference\")\n\n```\nbefore_model(state: StateT, runtime: before_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run before the model is called.\n\nAsync version is `abefore_model`\n\n#### abefore\\_model `async` [Â¶](#deepagents.SubAgentMiddleware.abefore_model \"Copy anchor link to this section for reference\")\n\n```\nabefore_model(state: StateT, runtime: abefore_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run before the model is called.\n\n#### after\\_model [Â¶](#deepagents.SubAgentMiddleware.after_model \"Copy anchor link to this section for reference\")\n\n```\nafter_model(state: StateT, runtime: after_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run after the model is called.\n\nAsync version is `aafter_model`\n\n#### aafter\\_model `async` [Â¶](#deepagents.SubAgentMiddleware.aafter_model \"Copy anchor link to this section for reference\")\n\n```\naafter_model(state: StateT, runtime: aafter_model(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run after the model is called.\n\n#### after\\_agent [Â¶](#deepagents.SubAgentMiddleware.after_agent \"Copy anchor link to this section for reference\")\n\n```\nafter_agent(state: StateT, runtime: after_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nLogic to run after the agent execution completes.\n\n#### aafter\\_agent `async` [Â¶](#deepagents.SubAgentMiddleware.aafter_agent \"Copy anchor link to this section for reference\")\n\n```\naafter_agent(state: StateT, runtime: aafter_agent(state: StateT StateT, runtime:             Runtime\n\n\n  \n      dataclass\n   (langgraph.runtime.Runtime)\">Runtime[ContextT]) -> dict[str, Any] | None \n```\n\nAsync logic to run after the agent execution completes.\n\n#### wrap\\_tool\\_call [Â¶](#deepagents.SubAgentMiddleware.wrap_tool_call \"Copy anchor link to this section for reference\")\n\n```\nwrap_tool_call( wrap_tool_call( request: ToolCallRequest, request: ToolCallRequest ToolCallRequest, handler: Callable[[ToolCallRequest], handler: Callable[[ToolCallRequest ToolCallRequest],             ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command], ) ->             ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command \n```\n\nIntercept tool execution for retries, monitoring, or modification.\n\nAsync version is `awrap_tool_call`\n\nMultiple middleware compose automatically (first defined = outermost).\n\nExceptions propagate unless `handle_tool_errors` is configured on `ToolNode`.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `request` | Tool call request with call `dict`, `BaseTool`, state, and runtime.  Access state via `request.state` and runtime via `request.runtime`.  **TYPE:** `ToolCallRequest` |\n| `handler` | `Callable` to execute the tool (can be called multiple times).  **TYPE:** `ToolCallRequest` ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\")] |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\") | `ToolMessage` or `Command` (the final result). |\n\nThe handler `Callable` can be invoked multiple times for retry logic.\n\nEach call to handler is independent and stateless.\n\nExamples:\n\nModify request before execution\n\n```\ndef wrap_tool_call(self, request, handler): def  wrap_tool_call(self, request, handler): modified_call = { modified_call ={ **request.tool_call, ** request. tool_call, \"args\": { \"args\":{ **request.tool_call[\"args\"], ** request. tool_call[\"args\"], \"value\": request.tool_call[\"args\"][\"value\"] * 2, \"value\": request. tool_call[\"args\"][\"value\"]* 2, }, }, } } request = request.override(tool_call=modified_call) request = request. override(tool_call = modified_call) return handler(request) return handler(request)\n```\n\nRetry on error (call handler multiple times)\n\n```\ndef wrap_tool_call(self, request, handler): def  wrap_tool_call(self, request, handler): for attempt in range(3): for attempt in range(3): try: try: result = handler(request) result = handler(request) if is_valid(result): if is_valid(result):  return result return result except Exception: except Exception: if attempt == 2: if attempt == 2:  raise raise  return result return result\n```\n\nConditional retry based on response\n\n```\ndef wrap_tool_call(self, request, handler): def  wrap_tool_call(self, request, handler): for attempt in range(3): for attempt in range(3): result = handler(request) result = handler(request) if isinstance(result, ToolMessage) and result.status != \"error\": if isinstance(result, ToolMessage) and result. status!= \"error\":  return result return result if attempt < 2: if attempt< 2:  continue continue  return result return result\n```\n\n#### awrap\\_tool\\_call `async` [Â¶](#deepagents.SubAgentMiddleware.awrap_tool_call \"Copy anchor link to this section for reference\")\n\n```\nawrap_tool_call( awrap_tool_call( request: ToolCallRequest, request: ToolCallRequest ToolCallRequest, handler: Callable[[ToolCallRequest], Awaitable[handler: Callable[[ToolCallRequest ToolCallRequest], Awaitable[            ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command]], ) ->             ToolMessage (langchain_core.messages.ToolMessage)\">ToolMessage |             Command\n\n\n  \n      dataclass\n   (langgraph.types.Command)\">Command \n```\n\nIntercept and control async tool execution via handler callback.\n\nThe handler callback executes the tool call and returns a `ToolMessage` or `Command`. Middleware can call the handler multiple times for retry logic, skip calling it to short-circuit, or modify the request/response. Multiple middleware compose with first in list as outermost layer.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `request` | Tool call request with call `dict`, `BaseTool`, state, and runtime.  Access state via `request.state` and runtime via `request.runtime`.  **TYPE:** `ToolCallRequest` |\n| `handler` | Async callable to execute the tool and returns `ToolMessage` or `Command`.  Call this to execute the tool.  Can be called multiple times for retry logic.  Can skip calling it to short-circuit.  **TYPE:** `ToolCallRequest` ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\")]] |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| ToolMessage (`langchain_core.messages.ToolMessage`)\">ToolMessage |  [Command  `dataclass`  (`langgraph.types.Command`)\">Command](../langgraph/types/#langgraph.types.Command \"<code class=\") | `ToolMessage` or `Command` (the final result). |\n\nThe handler `Callable` can be invoked multiple times for retry logic.\n\nEach call to handler is independent and stateless.\n\nExamples:\n\nAsync retry on error\n\n```\nasync def awrap_tool_call(self, request, handler): async def  awrap_tool_call(self, request, handler): for attempt in range(3): for attempt in range(3): try: try: result = await handler(request) result = await handler(request) if is_valid(result): if is_valid(result):  return result return result except Exception: except Exception: if attempt == 2: if attempt == 2:  raise raise  return result return result\n```\n\n```\nasync def awrap_tool_call(self, request, handler): async def  awrap_tool_call(self, request, handler): if cached := await get_cache_async(request): if cached:= await get_cache_async(request): return ToolMessage(content=cached, tool_call_id=request.tool_call[\"id\"]) return ToolMessage(content = cached, tool_call_id = request. tool_call[\"id\"]) result = await handler(request) result = await handler(request) await save_cache_async(request, result) await save_cache_async(request, result)  return result return result\n```\n\n#### \\_\\_init\\_\\_ [Â¶](#deepagents.SubAgentMiddleware.__init__ \"Copy anchor link to this section for reference\")\n\n```\n__init__( __init__( *, *, default_model: str | default_model: str | BaseChatModel, default_tools: Sequence[            BaseTool (langchain.tools.BaseTool)\">BaseTool | Callable | dict[str, Any]] | None = None, default_tools: Sequence[            BaseTool (langchain.tools.BaseTool)\">BaseTool BaseTool | Callable | dict[str, Any]] | None = None, default_middleware: list[            AgentMiddleware (langchain.agents.middleware.types.AgentMiddleware)\">AgentMiddleware] | None = None, default_middleware: list[            AgentMiddleware (langchain.agents.middleware.types.AgentMiddleware)\">AgentMiddleware AgentMiddleware] | None = None, default_interrupt_on: dict[str, bool |             InterruptOnConfig (langchain.agents.middleware.InterruptOnConfig)\">InterruptOnConfig] | None = None, default_interrupt_on: dict[str, bool |            InterruptOnConfig (langchain.agents.middleware.InterruptOnConfig)\">InterruptOnConfig InterruptOnConfig] | None = None, subagents: list[            SubAgent (deepagents.middleware.subagents.SubAgent)\">SubAgent |             CompiledSubAgent (deepagents.middleware.subagents.CompiledSubAgent)\">CompiledSubAgent] | None = None, subagents: list[            SubAgent (deepagents.middleware.subagents.SubAgent)\">SubAgent SubAgent |            CompiledSubAgent (deepagents.middleware.subagents.CompiledSubAgent)\">CompiledSubAgent CompiledSubAgent] | None = None, system_prompt: str | None = TASK_SYSTEM_PROMPT, system_prompt: str | None = TASK_SYSTEM_PROMPT TASK_SYSTEM_PROMPT, general_purpose_agent: bool = True, general_purpose_agent: bool = True, task_description: str | None = None, task_description: str | None = None,) -> None ) -> None\n```\n\nInitialize the SubAgentMiddleware.\n\n#### wrap\\_model\\_call [Â¶](#deepagents.SubAgentMiddleware.wrap_model_call \"Copy anchor link to this section for reference\")\n\n```\nwrap_model_call( wrap_model_call( request: request:             ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest, handler: Callable[[            ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest],             ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse] ) ->             ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse \n```\n\nUpdate the system prompt to include instructions on using subagents.\n\n#### awrap\\_model\\_call `async` [Â¶](#deepagents.SubAgentMiddleware.awrap_model_call \"Copy anchor link to this section for reference\")\n\n```\nawrap_model_call( awrap_model_call( request: request:             ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest, handler: Callable[[            ModelRequest\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelRequest)\">ModelRequest], Awaitable[            ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse]] ) ->             ModelResponse\n\n\n  \n      dataclass\n   (langchain.agents.middleware.types.ModelResponse)\">ModelResponse \n```\n\n(async) Update the system prompt to include instructions on using subagents.\n\n### create\\_deep\\_agent [Â¶](#deepagents.create_deep_agent \"Copy anchor link to this section for reference\")\n\n```\ncreate_deep_agent( create_deep_agent( model: str | model: str | BaseChatModel | None = None, tools: Sequence[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool | Callable | dict[str, Any]] | None = None, tools: Sequence[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool BaseTool | Callable | dict[str, Any]] | None = None, *, *, system_prompt: str | None = None, system_prompt: str | None = None, middleware: Sequence[            AgentMiddleware (langchain.agents.middleware.types.AgentMiddleware)\">AgentMiddleware] = (), middleware: Sequence[            AgentMiddleware (langchain.agents.middleware.types.AgentMiddleware)\">AgentMiddleware AgentMiddleware] =(), subagents: list[            SubAgent (deepagents.middleware.subagents.SubAgent)\">SubAgent |             CompiledSubAgent (deepagents.middleware.subagents.CompiledSubAgent)\">CompiledSubAgent] | None = None, subagents: list[            SubAgent (deepagents.middleware.subagents.SubAgent)\">SubAgent SubAgent |            CompiledSubAgent (deepagents.middleware.subagents.CompiledSubAgent)\">CompiledSubAgent CompiledSubAgent] | None = None, response_format: ResponseFormat | None = None, response_format: ResponseFormat ResponseFormat | None = None, context_schema: type[Any] | None = None, context_schema: type[Any] | None = None, checkpointer: Checkpointer | None = None, checkpointer: Checkpointer Checkpointer | None = None, store:             BaseStore (langgraph.store.base.BaseStore)\">BaseStore | None = None, store:            BaseStore (langgraph.store.base.BaseStore)\">BaseStore BaseStore | None = None, backend: BackendProtocol | BackendFactory | None = None, backend: BackendProtocol BackendProtocol | BackendFactory BackendFactory | None = None, interrupt_on: dict[str, bool |             InterruptOnConfig (langchain.agents.middleware.InterruptOnConfig)\">InterruptOnConfig] | None = None, interrupt_on: dict[str, bool |            InterruptOnConfig (langchain.agents.middleware.InterruptOnConfig)\">InterruptOnConfig InterruptOnConfig] | None = None, debug: bool = False, debug: bool = False, name: str | None = None, name: str | None = None, cache:             BaseCache (langgraph.cache.base.BaseCache)\">BaseCache | None = None, cache:            BaseCache (langgraph.cache.base.BaseCache)\">BaseCache BaseCache | None = None,) ->             CompiledStateGraph (langgraph.graph.state.CompiledStateGraph)\">CompiledStateGraph ) ->            CompiledStateGraph (langgraph.graph.state.CompiledStateGraph)\">CompiledStateGraph CompiledStateGraph\n```\n\nCreate a deep agent.\n\nThis agent will by default have access to a tool to write todos (write\\_todos), seven file and execution tools: ls, read\\_file, write\\_file, edit\\_file, glob, grep, execute, and a tool to call subagents.\n\nThe execute tool allows running shell commands if the backend implements SandboxBackendProtocol. For non-sandbox backends, the execute tool will return an error message.\n\n| PARAMETER | DESCRIPTION |\n| --- | --- |\n| `model` | The model to use. Defaults to Claude Sonnet 4.  **TYPE:** `str |`  BaseChatModel (`langchain_core.language_models.BaseChatModel`)\">BaseChatModel | None   **DEFAULT:** `None` |\n| `tools` | The tools the agent should have access to.  **TYPE:** `Sequence[` BaseTool (`langchain_core.tools.BaseTool`)\">BaseTool | [Callable](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable \"<code>collections.abc.Callable</code>\") | [dict](https://docs.python.org/3/library/stdtypes.html#dict)[[str](https://docs.python.org/3/library/stdtypes.html#str), [Any](https://docs.python.org/3/library/typing.html#typing.Any \"<code>typing.Any</code>\")]] | None   **DEFAULT:** `None` |\n| `system_prompt` | The additional instructions the agent should have. Will go in the system prompt.  **TYPE:** `str | None`   **DEFAULT:** `None` |\n| `middleware` | Additional middleware to apply after standard middleware.  **TYPE:** `Sequence[` AgentMiddleware (`langchain.agents.middleware.types.AgentMiddleware`)\">AgentMiddleware]   **DEFAULT:** `()` |\n| `subagents` | The subagents to use. Each subagent should be a dictionary with the following keys: - `name` - `description` (used by the main agent to decide whether to call the sub agent) - `prompt` (used as the system prompt in the subagent) - (optional) `tools` - (optional) `model` (either a LanguageModelLike instance or dict settings) - (optional) `middleware` (list of AgentMiddleware)  **TYPE:** `list[` SubAgent (`deepagents.middleware.subagents.SubAgent`)\">SubAgent |  [CompiledSubAgent (`deepagents.middleware.subagents.CompiledSubAgent`)\">CompiledSubAgent](#deepagents.CompiledSubAgent \"<code class=\")] | None   **DEFAULT:** `None` |\n| `response_format` | A structured output response format to use for the agent.  **TYPE:** `ResponseFormat`   **DEFAULT:** `None` |\n| `context_schema` | The schema of the deep agent.  **TYPE:** `type[Any] | None`   **DEFAULT:** `None` |\n| `checkpointer` | Optional checkpointer for persisting agent state between runs.  **TYPE:** `Checkpointer`   **DEFAULT:** `None` |\n| `store` | Optional store for persistent storage (required if backend uses StoreBackend).  **TYPE:**  BaseStore (`langgraph.store.base.BaseStore`)\">BaseStore | None   **DEFAULT:** `None` |\n| `backend` | Optional backend for file storage and execution. Pass either a Backend instance or a callable factory like `lambda rt: StateBackend(rt)`. For execution support, use a backend that implements SandboxBackendProtocol.  **TYPE:** `BackendProtocol BackendFactory`   **DEFAULT:** `None` |\n| `interrupt_on` | Optional Dict[str, bool | InterruptOnConfig] mapping tool names to interrupt configs.  **TYPE:** `dict[str, bool |`  InterruptOnConfig (`langchain.agents.middleware.InterruptOnConfig`)\">InterruptOnConfig] | None   **DEFAULT:** `None` |\n| `debug` | Whether to enable debug mode. Passed through to create\\_agent.  **TYPE:** `bool`   **DEFAULT:** `False` |\n| `name` | The name of the agent. Passed through to create\\_agent.  **TYPE:** `str | None`   **DEFAULT:** `None` |\n| `cache` | The cache to use for the agent. Passed through to create\\_agent.  **TYPE:**  BaseCache (`langgraph.cache.base.BaseCache`)\">BaseCache | None   **DEFAULT:** `None` |\n\n| RETURNS | DESCRIPTION |\n| --- | --- |\n| CompiledStateGraph (`langgraph.graph.state.CompiledStateGraph`)\">CompiledStateGraph | A configured deep agent. |\n\n "}, {"url": "https://abacus.ai/help/chatllm-ai-super-assistant/deepagent", "title": "Welcome to the Abacus AI Deep Agent Help Doc", "content": "## What is Abacus AI Deep Agent?â€‹. From building apps to analyzing data, writing reports to managing entire workflows, Abacus AI Deep Agent connects with your tools, performs real-time research, and delivers beautifully packaged results â€” all in one place. ## What Can Abacus AI Deep Agent Do?â€‹. Abacus AI Deep Agent now supports multiple capabilities, each designed to help you complete real tasks â€” end-to-end. Abacus AI Deep Agent lets you create powerful, production-ready **apps and websites** â€” with real databases, authentication, LLM capabilities, versioning, and seamless deployment. Abacus AI Deep Agent creates **polished, presentation-ready slides and documents** â€” in PowerPoint or PDF â€” with structured layouts, visuals, and compelling content. Abacus AI Deep Agent conducts real-time research using live web access â€” gathering trusted sources, extracting insights, and organizing them into structured, readable reports with visuals. * **All-in-One Generative Studio:** From video storytelling to image design â€” Abacus AI Deep Agent lets you visualize anything you imagine.", "score": 0.76300776, "raw_content": "[Skip to main content](#__docusaurus_skipToContent_fallback)\n\nðŸ“¢ Check out our latest [Platform Updates](/help/platform-updates)\n\n# Welcome to the Abacus AI Deep Agent Help Doc\n\nYour guide to unlocking the full power of the first god-tier, all-in-one AI agent.\n\nBrowse the sections below to explore everything Abacus AI Deep Agent can do â€” and how to use it effortlessly.\n\n---\n\n## Documentation Sections[â€‹](#documentation-sections \"Direct link to Documentation Sections\")\n\n* [What is Abacus AI Deep Agent?](#what-is-abacus-ai-deep-agent)\n* [What Can Abacus AI Deep Agent Do?](#what-can-abacus-ai-deep-agent-do)\n  1. [App & Website Creation](#1-app--website-creation)\n  2. [Automated AI Workflows](#2-automated-ai-workflows)\n  3. [AI Chatbots](#3-ai-chatbots)\n  4. [Presentations & Reports](#4-presentations--reports)\n  5. [Deep Research & Live Web Browsing](#5-deep-research--live-web-browsing)\n  6. [Data Analysis & Insights](#6-data-analysis--insights)\n  7. [Short Videos & AI-Generated Images](#7-short-videos--ai-generated-images)\n* [Prompting Best Practices](#prompting-best-practices)\n* [How Long Does It Take?](#how-long-does-it-take)\n* [Credits & Pro Tier](#credits--pro-tier)\n* [Still Need Help?](#still-need-help)\n\n---\n\n## What is Abacus AI Deep Agent?[â€‹](#what-is-abacus-ai-deep-agent \"Direct link to What is Abacus AI Deep Agent?\")\n\n**Abacus AI Deep Agent** is your multi-skilled AI agent that can not only think â€” it can **act**. From building apps to analyzing data, writing reports to managing entire workflows, Abacus AI Deep Agent connects with your tools, performs real-time research, and delivers beautifully packaged results â€” all in one place.\n\nWhether you're a creator, analyst, founder, or knowledge worker â€” Abacus AI Deep Agent empowers you to go from idea â†’ execution in minutes.\n\n---\n\n## What Can Abacus AI Deep Agent Do?[â€‹](#what-can-abacus-ai-deep-agent-do \"Direct link to What Can Abacus AI Deep Agent Do?\")\n\nAbacus AI Deep Agent now supports multiple capabilities, each designed to help you complete real tasks â€” end-to-end. Here's what's possible:\n\n### 1. App & Website Creation[â€‹](#1-app--website-creation \"Direct link to 1. App & Website Creation\")\n\n##### **What it does:**[â€‹](#what-it-does \"Direct link to what-it-does\")\n\nAbacus AI Deep Agent lets you create powerful, production-ready **apps and websites** â€” with real databases, authentication, LLM capabilities, versioning, and seamless deployment. Whether you're building a personal project, a startup MVP, or a smart internal tool, Abacus AI Deep Agent turns your ideas into launch-ready applications â€” fast.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities \"Direct link to key-capabilities\")\n\n* **Database + Auth Support:** Build apps with built-in database integration and secure, reliable authentication. No setup needed.\n* **LLM-Enabled Apps:** Add AI chat, document Q&A, or generation features to your app. Abacus AI Deep Agent can integrate LLMs right into the flow.\n* **Checkpointing & Deployments:** Make changes, preview versions, and deploy the one you like â€” instantly. Iterate at the speed of thought.\n* **Custom Domain Publishing:** Host your app on your own domain. Provide the domain on the Abacus AI Deep Agent platform and complete the simple integration â€” your app goes live with a custom URL in minutes.\n\n##### **How to use:**[â€‹](#how-to-use \"Direct link to how-to-use\")\n\n1. Ask Abacus AI Deep Agent to build an app or site (mention desired purpose or features).\n2. Specify if you need a database, API, LLM AI or chatbot embedded.\n3. Abacus AI Deep Agent will generate the code, preview it, and let you deploy with one click.\n4. More details can be found [here.](/help/chatllm-ai-super-assistant/deepagent-apps)\n\n---\n\n### 2. Automated AI Workflows[â€‹](#2-automated-ai-workflows \"Direct link to 2. Automated AI Workflows\")\n\n##### **What it does:**[â€‹](#what-it-does-1 \"Direct link to what-it-does-1\")\n\nSet up powerful, hands-free **AI workflows** that connect with your favorite tools â€” automate tasks, trigger actions, and schedule recurring jobs. Abacus AI Deep Agent can talk to your apps, run smart actions, and deliver outcomes â€” without you lifting a finger.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities-1 \"Direct link to key-capabilities-1\")\n\n* **Automated Workflow Creation:** Just describe what you want done. Abacus AI Deep Agent designs the entire AI workflow â€” logic, tool integration, and scheduling â€” for you.\n* **Rich Integration Support:** Abacus AI Deep Agent connects natively to tools like Slack, Gmail, and your internal systems.\n* **MCP Discovery Engine:** Need to interact with apps like **Github, Notion, Salesforce, JIRA, or Twitter**? Just prompt the task â€” Abacus AI Deep Agent finds and recommends the right MCP (Model Context Protocol) or tool to configure in an easy form-based setup.\n* **Custom API Support:** Want to connect to your own APIs? Just paste your token in the prompt or within the AI workflow â€” Abacus AI Deep Agent will handle the rest of the setup securely.\n* **Task Scheduling:** Automate recurring workflows! Want daily AI news in your inbox? Weekly Slack reports? Meeting summaries delivered every Friday? Just say the word.\n\n##### **How to use it:**[â€‹](#how-to-use-it \"Direct link to how-to-use-it\")\n\n1. Simply **describe the task** you want automated â€” what it should do, where, and how often (if needed).\n2. Abacus AI Deep Agent will identify any required integrations or MCPs.\n3. **Approve or connect** the suggested tool in one click.\n4. Let Abacus AI Deep Agent execute the workflow or schedule it based on your instructions.\n\nâœ¨ *No coding. No configuration. Just tell it what to do â€” and Abacus AI Deep Agent makes it real.*\n\n---\n\n### 3. AI Chatbots[â€‹](#3-ai-chatbots \"Direct link to 3. AI Chatbots\")\n\n##### **What it does:**[â€‹](#what-it-does-2 \"Direct link to what-it-does-2\")\n\nAbacus AI Deep Agent lets you build powerful AI chatbots that go far beyond answering questions. You can create assistants trained on your content, enable them to take actions, share them with your team, embed them on your website â€” and connect them to your tools to get work done.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities-2 \"Direct link to key-capabilities-2\")\n\n* **Knowledge-Driven Assistants:** Train your chatbot on websites, documents, PDFs, or internal wikis â€” no coding required.\n* **Share Anywhere:** Make the chatbot private for your team, public for anyone, or embed it directly on your website or product.\n* **Tool Integration for Actions:** Want your bot to **schedule meetings, send reports, read from Google Docs, or query Slack or Jira**? Just connect your system tools and Abacus AI Deep Agent enables your chatbot to act on them.\n\n##### **What you can create:**[â€‹](#what-you-can-create \"Direct link to what-you-can-create\")\n\n* Website help assistants, Shopping Assistants, FAQ/Support Agent, etc.\n\n##### **How to use:**[â€‹](#how-to-use-1 \"Direct link to how-to-use-1\")\n\n1. Ask Abacus AI Deep Agent to create a chatbot and share the knowledge source (e.g., website URL or document).\n2. Define where you want to deploy it (web, app, etc.).\n3. Abacus AI Deep Agent trains the bot and gives you a deployable widget or link.\n\n---\n\n### 4. Presentations & Reports[â€‹](#4-presentations--reports \"Direct link to 4. Presentations & Reports\")\n\n##### **What it does:**[â€‹](#what-it-does-3 \"Direct link to what-it-does-3\")\n\nAbacus AI Deep Agent creates **polished, presentation-ready slides and documents** â€” in PowerPoint or PDF â€” with structured layouts, visuals, and compelling content. Whether you're pitching, teaching, or reporting, just describe what you need, and Abacus AI Deep Agent builds it for you.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities-3 \"Direct link to key-capabilities-3\")\n\n* **Smart Slide Structuring:** Get up to **25 well-organized slides**, with relevant images, charts, flows, advanced slide layouts, and clean formatting.\n* **Use Your Data or Source from Web:** Upload your own data or ask Abacus AI Deep Agent to research content online to fill your deck.\n\n##### **How to use it:**[â€‹](#how-to-use-it-1 \"Direct link to how-to-use-it-1\")\n\n1. Prompt Abacus AI Deep Agent with your topic and intent (pitch, summary, explainer, etc.).\n2. Mention the format (PPT or PDF), tone, and preferred slide count.\n3. Download your deck or share it directly â€” no editing required.\n\n---\n\n### 5. Deep Research & Live Web Browsing[â€‹](#5-deep-research--live-web-browsing \"Direct link to 5. Deep Research & Live Web Browsing\")\n\n##### **What it does:**[â€‹](#what-it-does-4 \"Direct link to what-it-does-4\")\n\nAbacus AI Deep Agent conducts real-time research using live web access â€” gathering trusted sources, extracting insights, and organizing them into structured, readable reports with visuals.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities-4 \"Direct link to key-capabilities-4\")\n\n* **Live Web Access:** Includes the most up-to-date articles, reports, and web content\n* **In-Depth + Interdisciplinary Research:** Works across domains â€” tech, science, business, policy, etc.\n* **Visual-Enhanced Reports:** Pulls relevant images and media to enhance understanding\n* **Structured Outputs:** Includes citations, comparison tables, and concise takeaways\n\n##### **Perfect for:**[â€‹](#perfect-for \"Direct link to perfect-for\")\n\nTrend scouting, Market research, Technology exploration, Sourcing latest reports or references\n\n##### **How to use it:**[â€‹](#how-to-use-it-2 \"Direct link to how-to-use-it-2\")\n\n1. Ask your question or give a topic to research.\n2. Mention if you want sources, images, summaries, or comparisons.\n3. Abacus AI Deep Agent browses, analyzes, and returns a clean, structured research pack.\n\n---\n\n### 6. Data Analysis & Insights[â€‹](#6-data-analysis--insights \"Direct link to 6. Data Analysis & Insights\")\n\n##### **What it does:**[â€‹](#what-it-does-5 \"Direct link to what-it-does-5\")\n\nAbacus AI Deep Agent helps you uncover powerful insights from **spreadsheets, structured web data, and domain-specific datasets** â€” no code, no formulas. Upload files or point to online sources, and get rich analysis, visuals, and decision-ready recommendations.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities-5 \"Direct link to key-capabilities-5\")\n\n* **Upload or Extract Data:** Upload CSV/Excel files, or ask Abacus AI Deep Agent to scrape structured tables from websites and use them as input.\n* **Smart Analysis Across Domains:** Perform financial, spatial, market, or performance analytics based on your dataset's context. Abacus AI Deep Agent adapts the output to suit the domain.\n* **Bulk Data + LLM Actions:** Clean, reformat, or transform entire columns at once â€” or apply prompts (e.g., summarize, classify, rewrite) in bulk using LLMs.\n* **Insightful Dashboards:** Get beautiful, professional dashboards that visualize key insights, trends, and recommendations â€” ready to share or present.\n\n##### **How to use it:**[â€‹](#how-to-use-it-3 \"Direct link to how-to-use-it-3\")\n\n1. Upload your file or describe a web source to extract data from.\n2. Ask Abacus AI Deep Agent for the kind of analysis or transformation you want along with the output format (Sheet, Reports or Dashboards, etc.)\n3. Receive a dashboard, summary, or enhanced file â€” downloadable and presentation-ready.\n\n---\n\n### 7. Short Videos & AI-Generated Images[â€‹](#7-short-videos--ai-generated-images \"Direct link to 7. Short Videos & AI-Generated Images\")\n\n##### **What it does:**[â€‹](#what-it-does-6 \"Direct link to what-it-does-6\")\n\nCreate viral-ready **short videos** and stunning **images** with just a prompt. Abacus AI Deep Agent gives you access to the **industry's top media generation models** â€” all under one roof.\n\n##### **Key Capabilities:**[â€‹](#key-capabilities-6 \"Direct link to key-capabilities-6\")\n\n* **Generate Reels, Shorts & Explainers:** Instantly create platform-ready videos with script, scenes, visuals, sound, and even lip-sync.\n* **Create or Edit AI Images:** Generate realistic faces, graphics, marketing creatives, or product mockups â€” or edit any image with just a prompt.\n* **Script-to-Scene Flow:** Provide an idea and get back full scenes, narration, and visuals â€” ready to share.\n* **All-in-One Generative Studio:** From video storytelling to image design â€” Abacus AI Deep Agent lets you visualize anything you imagine.\n\n##### **How to use:**[â€‹](#how-to-use-2 \"Direct link to how-to-use-2\")\n\n1. Describe your video or image idea.\n2. Mention platform, tone, or visual style.\n3. Abacus AI Deep Agent returns a finished short video or high-res image â€” ready to post, embed, or reuse.\n\n*Stay at the top of your content game â€” Abacus AI Deep Agent makes creativity effortless.*\n\n---\n\n## Prompting Best Practices[â€‹](#prompting-best-practices \"Direct link to Prompting Best Practices\")\n\nTo get the most out of Abacus AI Deep Agent:\n\n* **Be clear and specific** about the task: what you want, how long, and what format.\n* For complex tasks, break down your task into multiple steps first and then start small and build more on top of it for the best results.\n* Mention if you want **sources, charts, visuals, or summaries**.\n* You can always say \"you decide\" if you want Abacus AI Deep Agent to make smart decisions.\n\n---\n\n## How Long Does It Take?[â€‹](#how-long-does-it-take \"Direct link to How Long Does It Take?\")\n\nMost tasks take **5â€“25 minutes** depending on complexity. Make sure your system is on and connected to the internet during the task.\n\n---\n\n## Credits & Pro Tier[â€‹](#credits--pro-tier \"Direct link to Credits & Pro Tier\")\n\n* Each Abacus AI Deep Agent task typically uses **500â€“1000 credits**.\n* On the **Basic Tier**, you get 3 Abacus AI Deep Agent tasks/month.\n* Upgrade to **Pro Tier ($10/month)** for:\n  + Unlimited task access (as long as you have credits)\n  + A more powerful version of Abacus AI Deep Agent\n  + +5000 bonus credits\n\n---\n\n## Still Need Help?[â€‹](#still-need-help \"Direct link to Still Need Help?\")\n\nBefore reaching out to support, check:\n\n* Are you using the right **category or prompt** for your goal?\n* Have you mentioned the **output format** clearly?\n* Have you connected your tools for automation?\n\nIf you're still stuck, [contact support](mailto:support@abacus.ai) â€” we're here for you.\n\n* [Documentation Sections](#documentation-sections)\n* [What is Abacus AI Deep Agent?](#what-is-abacus-ai-deep-agent)\n* [What Can Abacus AI Deep Agent Do?](#what-can-abacus-ai-deep-agent-do)\n  + [1. App & Website Creation](#1-app--website-creation)\n  + [2. Automated AI Workflows](#2-automated-ai-workflows)\n  + [3. AI Chatbots](#3-ai-chatbots)\n  + [4. Presentations & Reports](#4-presentations--reports)\n  + [5. Deep Research & Live Web Browsing](#5-deep-research--live-web-browsing)\n  + [6. Data Analysis & Insights](#6-data-analysis--insights)\n  + [7. Short Videos & AI-Generated Images](#7-short-videos--ai-generated-images)\n* [Prompting Best Practices](#prompting-best-practices)\n* [How Long Does It Take?](#how-long-does-it-take)\n* [Credits & Pro Tier](#credits--pro-tier)\n* [Still Need Help?](#still-need-help)"}], "response_time": 0.69, "request_id": "e3011357-28cd-48ba-9045-9c00931c35e3"}